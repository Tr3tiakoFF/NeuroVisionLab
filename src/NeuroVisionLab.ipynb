{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18, mobilenet_v2, vit_b_16\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for memory optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configure reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e52b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare model\n",
    "def prepare_model(model_name, num_classes, dataset_name):\n",
    "    \"\"\"\n",
    "    Prepare model with the correct number of classes\n",
    "    \n",
    "    Args:\n",
    "        model_name: model name ('resnet', 'mobilenet', 'vit')\n",
    "        num_classes: number of classes\n",
    "        \n",
    "    Returns:\n",
    "        model: prepared model\n",
    "    \"\"\"\n",
    "    if model_name == 'resnet':\n",
    "        model = resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # For single-channel images\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'mobilenet':\n",
    "        model = mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights.DEFAULT)\n",
    "        model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)  # For single-channel images\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "        \n",
    "    elif model_name == 'vit':\n",
    "        model = vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n",
    "        model.conv_proj = nn.Conv2d(1, 768, kernel_size=16, stride=16)  # For single-channel images\n",
    "        model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # For SVHN (RGB), we need to change the first layer\n",
    "    if dataset_name == 'svhn':  # Explicitly check for SVHN dataset\n",
    "        if model_name == 'resnet':\n",
    "            model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        elif model_name == 'mobilenet':\n",
    "            model.features[0][0] = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        elif model_name == 'vit':\n",
    "            model.conv_proj = nn.Conv2d(3, 768, kernel_size=16, stride=16)\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf06154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare datasets\n",
    "def prepare_datasets(dataset_name, batch_size=32):  # Reduced batch size\n",
    "    \"\"\"\n",
    "    Prepare datasets for training and evaluation\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: dataset name ('emnist', 'kmnist', 'svhn')\n",
    "        batch_size: batch size\n",
    "        \n",
    "    Returns:\n",
    "        train_loader: training data loader\n",
    "        val_loader: validation data loader\n",
    "        test_loader: test data loader\n",
    "        num_classes: number of classes in the dataset\n",
    "    \"\"\"\n",
    "    if dataset_name == 'emnist':\n",
    "        # EMNIST dataset (letters)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1751,), (0.3331,))  # Normalization for EMNIST\n",
    "        ])\n",
    "        \n",
    "        train_dataset = torchvision.datasets.EMNIST(\n",
    "            root='./data', split='letters', train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.EMNIST(\n",
    "            root='./data', split='letters', train=False, download=True, transform=transform\n",
    "        )\n",
    "\n",
    "        train_dataset.targets -= 1\n",
    "        test_dataset.targets -= 1\n",
    "        \n",
    "        # Split training dataset into training and validation\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "        \n",
    "        num_classes = 26  # A-Z (26 classes)\n",
    "        \n",
    "    elif dataset_name == 'kmnist':\n",
    "        # KMNIST dataset (Kuzushiji-MNIST)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1918,), (0.3483,))  # Normalization values for KMNIST\n",
    "        ])\n",
    "        \n",
    "        # Load KMNIST\n",
    "        train_dataset = torchvision.datasets.KMNIST(\n",
    "            root='./data', train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.KMNIST(\n",
    "            root='./data', train=False, download=True, transform=transform\n",
    "        )\n",
    "        \n",
    "        # Split training dataset into training and validation\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "        \n",
    "        num_classes = 10  # KMNIST has 10 classes\n",
    "        \n",
    "    elif dataset_name == 'svhn':\n",
    "        # SVHN dataset\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4380, 0.4440, 0.4730), (0.1751, 0.1771, 0.1744))  # Normalization for SVHN\n",
    "        ])\n",
    "        \n",
    "        train_dataset = torchvision.datasets.SVHN(\n",
    "            root='./data', split='train', download=True, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.SVHN(\n",
    "            root='./data', split='test', download=True, transform=transform\n",
    "        )\n",
    "        \n",
    "        # Split training dataset into training and validation\n",
    "        train_size = int(0.8 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "        \n",
    "        num_classes = 10  # Digits 0-9\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b62dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate metrics (accuracy, precision, recall, f1-score)\n",
    "def calculate_metrics(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: true labels\n",
    "        y_pred: predicted labels\n",
    "        num_classes: number of classes\n",
    "        \n",
    "    Returns:\n",
    "        metrics: dictionary with metrics (accuracy, precision, recall, f1-score)\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays if needed\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if isinstance(y_pred, torch.Tensor):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['accuracy'] = (y_true == y_pred).mean() * 100\n",
    "    \n",
    "    # Precision, Recall, and F1-Score (with different averaging methods)\n",
    "    metrics['precision_micro'] = precision_score(y_true, y_pred, average='micro', zero_division=0) * 100\n",
    "    metrics['recall_micro'] = recall_score(y_true, y_pred, average='micro', zero_division=0) * 100\n",
    "    metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro', zero_division=0) * 100\n",
    "    \n",
    "    metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
    "    metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
    "    \n",
    "    metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0) * 100\n",
    "    \n",
    "    # Get per class metrics\n",
    "    per_class_metrics = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store confusion matrix\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return metrics, per_class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count trainable parameters in a model\n",
    "def count_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in a model\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "\n",
    "    Returns:\n",
    "        total_params: number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute model size in megabytes\n",
    "def get_model_size_mb(model):\n",
    "    \"\"\"\n",
    "    Calculate the memory footprint of the model in megabytes (MB)\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "\n",
    "    Returns:\n",
    "        size_mb: float, total size of model parameters and buffers in MB\n",
    "    \"\"\"\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "    size_mb = (param_size + buffer_size) / 1024 ** 2\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training and evaluating the model with memory optimization\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, epochs=5, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model\n",
    "    \n",
    "    Args:\n",
    "        model: model to train\n",
    "        train_loader: training data loader\n",
    "        val_loader: validation data loader\n",
    "        test_loader: test data loader\n",
    "        epochs: number of epochs\n",
    "        lr: learning rate\n",
    "        \n",
    "    Returns:\n",
    "        results: dictionary with results (loss, accuracy, time, precision, recall, f1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log model statistics\n",
    "    print(f\"\\nModel statistics:\")\n",
    "    print(f\"- Trainable parameters: {count_trainable_parameters(model):,}\")\n",
    "    print(f\"- Model size: {get_model_size_mb(model):.2f} MB\")\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # Setup loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Get number of classes\n",
    "    try:\n",
    "        num_classes = model.fc.out_features  # For ResNet\n",
    "    except:\n",
    "        try:\n",
    "            num_classes = model.classifier[1].out_features  # For MobileNet\n",
    "        except:\n",
    "            try:\n",
    "                num_classes = model.heads.head.out_features  # For ViT\n",
    "            except:\n",
    "                num_classes = model.head.out_features\n",
    "    \n",
    "    # Track results\n",
    "    results = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': [],\n",
    "        'test_loss': None,\n",
    "        'test_acc': None,\n",
    "        'test_precision': None,\n",
    "        'test_recall': None,\n",
    "        'test_f1': None,\n",
    "        'training_time': 0,\n",
    "        'class_metrics': None,\n",
    "        'confusion_matrix': None\n",
    "    }\n",
    "    \n",
    "    # Start training time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\") as train_bar:\n",
    "            for inputs, labels in train_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update statistics\n",
    "                train_loss += loss.item()\n",
    "                train_bar.set_postfix(loss=train_loss/len(train_bar))\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, labels, outputs, loss\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "            peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            print(f\"Epoch {epoch+1} Train VRAM usage: Current {current_mem:.2f} MB, Peak {peak_mem:.2f} MB\")\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        else:\n",
    "            mem_cpu = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "            print(f\"Epoch {epoch+1} Train CPU RAM usage: {mem_cpu:.2f} MB\")\n",
    "\n",
    "        # Calculate average loss on training set\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        results['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\") as val_bar:\n",
    "                for inputs, labels in val_bar:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Update statistics\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    \n",
    "                    # Collect labels and predictions for metric calculation\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    \n",
    "                    val_bar.set_postfix(loss=val_loss/len(val_bar))\n",
    "                    \n",
    "                    # Clear memory\n",
    "                    del inputs, labels, outputs, loss\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            current_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "            peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            print(f\"Epoch {epoch+1} Val VRAM usage: Current {current_mem:.2f} MB, Peak {peak_mem:.2f} MB\")\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        else:\n",
    "            mem_cpu = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "            print(f\"Epoch {epoch+1} Val CPU RAM usage: {mem_cpu:.2f} MB\")\n",
    "\n",
    "        # Calculate average loss and metrics on validation set\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_metrics, _ = calculate_metrics(np.array(all_labels), np.array(all_preds), num_classes)\n",
    "        \n",
    "        # Store validation results\n",
    "        results['val_loss'].append(avg_val_loss)\n",
    "        results['val_acc'].append(val_metrics['accuracy'])\n",
    "        results['val_precision'].append(val_metrics['precision_weighted'])\n",
    "        results['val_recall'].append(val_metrics['recall_weighted'])\n",
    "        results['val_f1'].append(val_metrics['f1_weighted'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Val Metrics: Acc: {val_metrics['accuracy']:.2f}%, Precision: {val_metrics['precision_weighted']:.2f}%, \" +\n",
    "              f\"Recall: {val_metrics['recall_weighted']:.2f}%, F1: {val_metrics['f1_weighted']:.2f}%\")\n",
    "        \n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Total training time\n",
    "    results['training_time'] = time.time() - start_time\n",
    "    print(f\"Training completed in {results['training_time']:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, desc=\"Testing\") as test_bar:\n",
    "            for inputs, labels in test_bar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Update statistics\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Collect labels and predictions for metric calculation\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                \n",
    "                test_bar.set_postfix(loss=test_loss/len(test_bar))\n",
    "                \n",
    "                # Clear memory\n",
    "                del inputs, labels, outputs, loss\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate average loss and metrics on test set\n",
    "    results['test_loss'] = test_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_metrics, class_metrics = calculate_metrics(np.array(all_labels), np.array(all_preds), num_classes)\n",
    "    \n",
    "    # Store test results\n",
    "    results['test_acc'] = test_metrics['accuracy']\n",
    "    results['test_precision'] = test_metrics['precision_weighted']\n",
    "    results['test_recall'] = test_metrics['recall_weighted']\n",
    "    results['test_f1'] = test_metrics['f1_weighted']\n",
    "    results['confusion_matrix'] = test_metrics['confusion_matrix']\n",
    "    results['class_metrics'] = class_metrics\n",
    "    \n",
    "    print(f\"Test Loss: {results['test_loss']:.4f}\")\n",
    "\n",
    "    print(f\"Test Metrics: Acc: {results['test_acc']:.2f}%, Precision: {results['test_precision']:.2f}%, \" +\n",
    "          f\"Recall: {results['test_recall']:.2f}%, F1: {results['test_f1']:.2f}%\")\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        peak_memory_mb = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        print(f\"Peak VRAM used: {peak_memory_mb:.2f} MB\")\n",
    "        results['peak_vram'] = peak_memory_mb\n",
    "    else:\n",
    "        results['peak_vram'] = None\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare models on a given dataset\n",
    "def compare_models_on_dataset(dataset_name, models=['resnet', 'mobilenet', 'vit'], \n",
    "                             epochs=3, batch_size=32, lr=0.001):  # Reduced batch size\n",
    "    \"\"\"\n",
    "    Compare models on a given dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: dataset name ('emnist', 'kmnist', 'svhn')\n",
    "        models: list of models to compare\n",
    "        epochs: number of epochs\n",
    "        batch_size: batch size\n",
    "        lr: learning rate\n",
    "        \n",
    "    Returns:\n",
    "        comparison_results: dictionary with comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Comparing models on dataset {dataset_name.upper()} {'='*20}\\n\")\n",
    "    \n",
    "    # Prepare datasets\n",
    "    train_loader, val_loader, test_loader, num_classes = prepare_datasets(dataset_name, batch_size)\n",
    "    \n",
    "    # Evaluate each model\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"\\n{'-'*10} Model: {model_name.upper()} {'-'*10}\")\n",
    "        \n",
    "        # Prepare model\n",
    "        model = prepare_model(model_name, num_classes, dataset_name)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        results = train_and_evaluate(model, train_loader, val_loader, test_loader, epochs, lr)\n",
    "        \n",
    "        # Save results\n",
    "        comparison_results[model_name] = results\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f971a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(conf_matrix, class_names, title='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        conf_matrix: confusion matrix\n",
    "        class_names: list of class names\n",
    "        title: title for the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Display values in cells\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot comparison results\n",
    "def plot_comparison_results(all_results):\n",
    "    \"\"\"\n",
    "    Plot comparison charts\n",
    "    \n",
    "    Args:\n",
    "        all_results: dictionary with comparison results for all datasets\n",
    "    \"\"\"\n",
    "    datasets = list(all_results.keys())\n",
    "    models = list(all_results[datasets[0]].keys())\n",
    "    \n",
    "    # Prepare data for comparison tables\n",
    "    test_acc_data = {dataset: {model: all_results[dataset][model]['test_acc'] for model in models} for dataset in datasets}\n",
    "    test_precision_data = {dataset: {model: all_results[dataset][model]['test_precision'] for model in models} for dataset in datasets}\n",
    "    test_recall_data = {dataset: {model: all_results[dataset][model]['test_recall'] for model in models} for dataset in datasets}\n",
    "    test_f1_data = {dataset: {model: all_results[dataset][model]['test_f1'] for model in models} for dataset in datasets}\n",
    "    training_time_data = {dataset: {model: all_results[dataset][model]['training_time'] for model in models} for dataset in datasets}\n",
    "    \n",
    "    # Plot metrics charts\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Test accuracy chart\n",
    "    plt.subplot(3, 2, 1)\n",
    "    df_test_acc = pd.DataFrame(test_acc_data)\n",
    "    df_test_acc.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Test Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Test precision chart\n",
    "    plt.subplot(3, 2, 2)\n",
    "    df_test_precision = pd.DataFrame(test_precision_data)\n",
    "    df_test_precision.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Test Precision (Weighted) (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Precision (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Test recall chart\n",
    "    plt.subplot(3, 2, 3)\n",
    "    df_test_recall = pd.DataFrame(test_recall_data)\n",
    "    df_test_recall.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Test Recall (Weighted) (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Recall (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Test F1 chart\n",
    "    plt.subplot(3, 2, 4)\n",
    "    df_test_f1 = pd.DataFrame(test_f1_data)\n",
    "    df_test_f1.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Test F1 Score (Weighted) (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('F1 Score (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Training time chart\n",
    "    plt.subplot(3, 2, 5)\n",
    "    df_time = pd.DataFrame(training_time_data)\n",
    "    df_time.plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Training Time (seconds)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Validation metrics during training\n",
    "    plt.subplot(3, 2, 6)\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            plt.plot(all_results[dataset][model]['val_f1'], label=f\"{dataset}-{model} (F1)\")\n",
    "    plt.title('Validation F1 Score During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison_metrics.png')  # Save the figure\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Plot confusion matrices for all models\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            if 'confusion_matrix' in all_results[dataset][model]:\n",
    "                conf_matrix = all_results[dataset][model]['confusion_matrix']\n",
    "\n",
    "                # Get class names based on dataset\n",
    "                if dataset == 'emnist':\n",
    "                    class_names = [chr(i + ord('A')) for i in range(26)]  # A-Z\n",
    "                elif dataset in ['kmnist', 'svhn']:\n",
    "                    class_names = [str(i) for i in range(10)]  # 0-9\n",
    "                else:\n",
    "                    class_names = [str(i) for i in range(conf_matrix.shape[0])]\n",
    "\n",
    "                # Plot and save confusion matrix\n",
    "                plt_cm = plot_confusion_matrix(\n",
    "                    conf_matrix,\n",
    "                    class_names,\n",
    "                    f'Confusion Matrix for {model.upper()} on {dataset.upper()}'\n",
    "                )\n",
    "                plt_cm.savefig(f'{dataset}_{model}_confusion_matrix.png')\n",
    "                plt_cm.show()\n",
    "\n",
    "\n",
    "    # Create comparison table\n",
    "    comparison_table = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            result = all_results[dataset][model]\n",
    "            key = f\"{dataset}-{model}\"\n",
    "            comparison_table[key] = {\n",
    "                'Test Accuracy (%)': result['test_acc'],\n",
    "                'Test Precision (%)': result['test_precision'],\n",
    "                'Test Recall (%)': result['test_recall'],\n",
    "                'Test F1 Score (%)': result['test_f1'],\n",
    "                'Training Time (s)': result['training_time']\n",
    "            }\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_table).T\n",
    "    print(\"\\nComparison table of all models on all datasets:\")\n",
    "    print(df_comparison)\n",
    "    \n",
    "    return df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf64c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the comparison\n",
    "def run_models_comparison(datasets=['emnist', 'kmnist', 'svhn'], \n",
    "                          models=['resnet', 'mobilenet', 'vit'],\n",
    "                          epochs=3, batch_size=32, lr=0.001):  # Reduced batch size\n",
    "    \"\"\"\n",
    "    Run model comparison on all datasets\n",
    "    \n",
    "    Args:\n",
    "        datasets: list of datasets to compare\n",
    "        models: list of models to compare\n",
    "        epochs: number of epochs\n",
    "        batch_size: batch size\n",
    "        lr: learning rate\n",
    "    \n",
    "    Returns:\n",
    "        df_comparison: table with comparison results\n",
    "    \"\"\"\n",
    "    # Save results for all datasets\n",
    "    all_results = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        # Compare models on current dataset\n",
    "        results = compare_models_on_dataset(dataset, models, epochs, batch_size, lr)\n",
    "        all_results[dataset] = results\n",
    "        \n",
    "        # Clear memory between datasets\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Plot comparison charts\n",
    "    df_comparison = plot_comparison_results(all_results)\n",
    "    \n",
    "    # Save detailed per-class metrics to CSV files\n",
    "    for dataset in datasets:\n",
    "        for model in models:\n",
    "            if 'class_metrics' in all_results[dataset][model]:\n",
    "                class_metrics_df = pd.DataFrame(all_results[dataset][model]['class_metrics'])\n",
    "                class_metrics_df.to_csv(f\"{dataset}_{model}_class_metrics.csv\")\n",
    "    \n",
    "    return df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b0565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a smaller comparison to save memory\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df_results = run_models_comparison(\n",
    "    datasets=['emnist'],\n",
    "    models=['resnet', 'mobilenet', 'vit'],\n",
    "    epochs=15,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee499ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df_results = run_models_comparison(\n",
    "    datasets=['kmnist'],\n",
    "    models=['resnet', 'mobilenet', 'vit'],\n",
    "    epochs=15,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a41fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df_results = run_models_comparison(\n",
    "    datasets=['svhn'],\n",
    "    models=['resnet', 'mobilenet', 'vit'],\n",
    "    epochs=15,\n",
    "    batch_size=32\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
